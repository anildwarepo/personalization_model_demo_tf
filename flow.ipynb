{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Normalization, CategoryEncoding\nfrom tensorflow.keras.optimizers import Adam\nimport joblib\nimport json\nimport os\n\n# Step 1: Data Generation\nnp.random.seed(42)\nnum_samples = 100000\ncontinuous_features = [f'cont_{i}' for i in range(150)]\ncategorical_features = [f'cat_{i}' for i in range(50)]\ncardinalities = [np.random.randint(2, 20) for _ in range(50)]  # random cardinality for each categorical feature\n\n# Generate continuous and categorical data\ndata = pd.DataFrame({f: np.random.rand(num_samples) * np.random.randint(1, 100) for f in continuous_features})\nfor i, f in enumerate(categorical_features):\n    data[f] = np.random.randint(0, cardinalities[i], size=num_samples)\n\n# Generate binary target variable\ndata['Y'] = np.random.randint(0, 2, size=num_samples)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['Y']), data['Y'], test_size=0.2, random_state=42)\n\n# Save transformations for AzureML\ntransformations = {\n    \"continuous\": [{\"name\": f\"cont_{i}\", \"type\": \"normalization\"} for i in range(150)],\n    \"categorical\": [{\"name\": f\"cat_{i}\", \"type\": \"one_hot_encoding\", \"cardinality\": cardinalities[i]} for i in range(50)]\n}\n\n# Step 2: Preprocessing Layers\nnumerical_inputs = [Input(shape=(1,), name=f) for f in continuous_features]\ncategorical_inputs = [Input(shape=(1,), name=f, dtype=\"int32\") for f in categorical_features]\n\n# Apply normalization to continuous features\nnormalized_numerical = [Normalization()(num_input) for num_input in numerical_inputs]\n\n# Apply one-hot encoding to categorical features\nencoded_categorical = [CategoryEncoding(num_tokens=card)(cat_input) for cat_input, card in zip(categorical_inputs, cardinalities)]\n\n# Concatenate all processed inputs\nprocessed_inputs = Concatenate()(normalized_numerical + encoded_categorical)\n\n# Step 3: Model Definition\nx = Dense(64, activation=\"relu\")(processed_inputs)\nx = Dense(32, activation=\"relu\")(x)\noutput = Dense(1, activation=\"sigmoid\")(x)\n\n# Build the model\nmodel = Model(inputs=numerical_inputs + categorical_inputs, outputs=output)\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\n# Step 4: Training the Model\n# Prepare input data in dictionary format for Keras\nX_train_dict = {name: X_train[name].values for name in continuous_features + categorical_features}\nX_test_dict = {name: X_test[name].values for name in continuous_features + categorical_features}\n\n# Train the model\nmodel.fit(X_train_dict, y_train, epochs=5, batch_size=128, validation_split=0.2)\n\n# Step 5: Save the model in SavedModel format for Azure ML deployment\nmodel_save_path = 'azure_deployable_model.keras'\nmodel.save(model_save_path)\n\n# Save transformations as a JSON file for reference\nwith open(\"transformations.json\", \"w\") as f:\n    json.dump(transformations, f)\n\n# Example scoring script for Azure ML (save as score.py)\nscore_script = \"\"\"\nimport json\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import load_model\n\ndef init():\n    global model\n    global transformations\n\n    # Load the trained Keras model\n    model = load_model('azure_deployable_model.keras')\n    \n    # Load transformations\n    with open('transformations.json', 'r') as f:\n        transformations = json.load(f)\n\ndef preprocess(data):\n    # Convert JSON data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Apply transformations based on metadata\n    processed_data = {}\n    \n    # Normalize continuous variables\n    for feature in transformations[\"continuous\"]:\n        processed_data[feature[\"name\"]] = (df[feature[\"name\"]] - df[feature[\"name\"]].mean()) / df[feature[\"name\"]].std()\n\n    # One-hot encode categorical variables\n    for feature in transformations[\"categorical\"]:\n        one_hot = pd.get_dummies(df[feature[\"name\"]], prefix=feature[\"name\"], drop_first=True)\n        for col in one_hot.columns:\n            processed_data[col] = one_hot[col]\n    \n    return pd.DataFrame(processed_data)\n\ndef run(raw_data):\n    # Parse the input JSON data\n    data = json.loads(raw_data)['data']\n    \n    # Preprocess data\n    input_data = preprocess(data)\n\n    # Predict using the model\n    predictions = model.predict(input_data)\n    \n    # Return predictions\n    return {\"predictions\": predictions.tolist()}\n\"\"\"\n\n# Write the scoring script to a file\nwith open(\"score.py\", \"w\") as f:\n    f.write(score_script)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:07:03.064341Z","iopub.execute_input":"2024-11-06T13:07:03.064789Z","iopub.status.idle":"2024-11-06T13:07:36.156472Z","shell.execute_reply.started":"2024-11-06T13:07:03.064747Z","shell.execute_reply":"2024-11-06T13:07:36.155535Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.5023 - loss: 1.1492 - val_accuracy: 0.4974 - val_loss: 0.6985\nEpoch 2/5\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.5029 - loss: 0.6988 - val_accuracy: 0.4970 - val_loss: 0.6985\nEpoch 3/5\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5033 - loss: 0.6980 - val_accuracy: 0.4989 - val_loss: 0.6970\nEpoch 4/5\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5059 - loss: 0.6961 - val_accuracy: 0.4996 - val_loss: 0.7037\nEpoch 5/5\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.4993 - loss: 0.6964 - val_accuracy: 0.5002 - val_loss: 0.6949\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:06:55.558934Z","iopub.execute_input":"2024-11-06T13:06:55.559844Z","iopub.status.idle":"2024-11-06T13:06:56.597574Z","shell.execute_reply.started":"2024-11-06T13:06:55.559796Z","shell.execute_reply":"2024-11-06T13:06:56.596114Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!rm -rf *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:05:54.966259Z","iopub.execute_input":"2024-11-06T13:05:54.966944Z","iopub.status.idle":"2024-11-06T13:05:56.010138Z","shell.execute_reply.started":"2024-11-06T13:05:54.966902Z","shell.execute_reply":"2024-11-06T13:05:56.008613Z"}},"outputs":[],"execution_count":2}]}